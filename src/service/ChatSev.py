from typing import Iterable, Optional

from dotenv import load_dotenv
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import AIMessageChunk
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import AddableDict
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables.utils import Output

from src.utils.Knowledge import Knowledge

# utils
from src.utils.llm_modle import get_llms

load_dotenv()


class ChatSev:
    _chat_history = ChatMessageHistory()  # å¯¹è¯å†å²

    def __init__(
        self, knowledge: Optional[Knowledge], chat_history_max_length: Optional[int] = 8
    ):
        self.knowledge: Optional[Knowledge] = knowledge
        self.chat_history_max_length: int = (
            chat_history_max_length if chat_history_max_length is not None else 8
        )

        self.knowledge_prompt = None  # é—®ç­”æ¨¡æ¿
        self.normal_prompt = None  # æ­£å¸¸æ¨¡æ¿
        self.create_chat_prompt()  # åˆ›å»ºèŠå¤©æ¨¡æ¿

    def create_chat_prompt(self) -> None:
        ai_info = "ä½ å«è¶…çº§æ— æ•Œéœ¸ç‹é¾™ğŸ¦–ï¼Œä¸€ä¸ªå¸®åŠ©äººä»¬è§£ç­”å„ç§é—®é¢˜çš„åŠ©æ‰‹ã€‚"

        # AIç³»ç»Ÿprompt
        knowledge_system_prompt = (
            f"{ai_info} å½“ç”¨æˆ·å‘ä½ æé—®ï¼Œè¯·ä½ ä½¿ç”¨ä¸‹é¢æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰é—®é¢˜çš„ç­”æ¡ˆï¼Œè¯·ä½ ç›´æ¥å›ç­”ä¸çŸ¥é“ã€‚æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å¦‚ä¸‹ï¼š\n\n"
            "{context}"
        )

        self.knowledge_prompt = ChatPromptTemplate.from_messages(  # çŸ¥è¯†åº“prompt
            [
                ("system", knowledge_system_prompt),
                ("placeholder", "{chat_history}"),
                ("human", "{input}"),
            ]
        )

        # æ²¡æœ‰æŒ‡å®šçŸ¥è¯†åº“çš„æ¨¡æ¿çš„AIç³»ç»Ÿæ¨¡æ¿
        self.normal_prompt = ChatPromptTemplate.from_messages(  # æ­£å¸¸prompt
            [
                ("system", ai_info),
                ("placeholder", "{chat_history}"),
                ("human", "{input}"),
            ]
        )

    @staticmethod
    def streaming_parse(chunks: Iterable[AIMessageChunk]) -> list[AddableDict]:
        """ç»Ÿä¸€æ¨¡å‹çš„è¾“å‡ºæ ¼å¼ï¼Œå°†æ¨¡å‹çš„è¾“å‡ºå­˜å‚¨åˆ°å­—å…¸answerçš„valueä¸­"""
        for chunk in chunks:
            yield AddableDict({"answer": chunk.content})

    def get_chain(
        self,
        api_key: Optional[str],
        collection: str,
        supplier: str,
        model: str,
        max_length: int,
        temperature: float = 0.8,
    ) -> RunnableWithMessageHistory:
        """è·å–èŠå¤©é“¾"""
        # chat = ChatOpenAI(model=model, max_tokens=max_length, temperature=temperature)
        chat = get_llms(
            supplier=supplier,
            model=model,
            api_key=api_key,
            max_length=max_length,
            temperature=temperature,
        )

        # ç»Ÿä¸€è¿”å›é€»è¾‘
        rag_chain = (
            self.normal_prompt | chat | self.streaming_parse
            if collection is None
            else create_retrieval_chain(
                self.knowledge.get_retrievers(collection),
                create_stuff_documents_chain(chat, self.knowledge_prompt),
            )
        )

        return RunnableWithMessageHistory(
            rag_chain,  # ä¼ å…¥èŠå¤©é“¾
            # lambda session_id: self._chat_history,
            get_session_history=self.get_session_chat_history,  # ä¼ å…¥å†å²ä¿¡æ¯
            input_messages_key="input",  # è¾“å…¥ä¿¡æ¯çš„é”®å
            history_messages_key="chat_history",  # å†å²ä¿¡æ¯çš„é”®å
            output_messages_key="answer",  # è¾“å‡ºç­”æ¡ˆ
        )

    def get_session_chat_history(self):
        return self._chat_history

    def invoke(
        self,
        question: str,
        api_key: Optional[str],
        collection: Optional[str],
        supplier: str,
        model: str,
        max_length=None,
        temperature=0.8,
    ) -> Output:
        """
        :param question: ç”¨æˆ·æå‡ºçš„é—®é¢˜ ä¾‹å¦‚: 'è¯·é—®ä½ æ˜¯è°ï¼Ÿ'
        :param collection: çŸ¥è¯†åº“æ–‡ä»¶åç§° ä¾‹å¦‚:'äººäº‹ç®¡ç†æµç¨‹.docx'
        :param model: ä½¿ç”¨æ¨¡å‹,é»˜è®¤ä¸º 'gpt-3.5-turbo'
        :param max_length: æ•°æ®è¿”å›æœ€å¤§é•¿åº¦
        :param temperature: æ•°æ®æ¸©åº¦å€¼
        """
        return self.get_chain(
            api_key, collection, supplier, model, max_length, temperature
        ).invoke({"input": question})

    def clear_history(self) -> None:
        """æ¸…é™¤å†å²ä¿¡æ¯"""
        self._chat_history.clear()

    def get_history_message(self) -> list:
        """è·å–å†å²ä¿¡æ¯"""
        return self._chat_history.messages
